{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epitran import Epitran\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "from pyxdameraulevenshtein import normalized_damerau_levenshtein_distance\n",
    "import textdistance\n",
    "from monge_eklan import MongeEklan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Перевод латиницы в кириллицу на основе Epitran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(string):\n",
    "    \n",
    "    res = epi.transliterate(string)\n",
    "\n",
    "    regular_dict = {\n",
    "        \"m\": \"м\",\n",
    "        \"e\": \"е\",\n",
    "        '̩': \"\",\n",
    "        \"l\": \"л\",\n",
    "        'ɪ': \"и\",\n",
    "        \"s\": \"с\",\n",
    "        \"f\": \"ф\",\n",
    "        \"n\": \"н\",\n",
    "        \"t\": \"т\",\n",
    "        \"i\": \"и\",\n",
    "        \"p\": \"п\",\n",
    "        \"k\": \"к\",\n",
    "        \"ɔ\": \"о\",\n",
    "        \"o\": \"о\",\n",
    "        \"ɑ\": \"о\",\n",
    "        \"p\": \"п\",\n",
    "        \"d\": \"д\",\n",
    "        \"ɡ\": \"г\",\n",
    "        \"ɛ\": \"е\",\n",
    "        \"z\": \"з\",\n",
    "        \"ŋ\": \"нг\",\n",
    "        \"v\": \"в\",\n",
    "        \"b\": \"б\",\n",
    "        'ʌ': \"а\",\n",
    "        'ʤ': \"дж\",\n",
    "        'ʒ': \"дж\",\n",
    "        'u': \"у\",\n",
    "        \"ʃ\":\"ш\",\n",
    "        'æ': \"е\",\n",
    "        'j':\"й\",\n",
    "        'ɹ':\"ер\",\n",
    "        \"ə\":\"е\",\n",
    "        \"w\":\"у\",\n",
    "        \"h\":\"х\",\n",
    "        \"ʧ\":\"ч\",\n",
    "        'a':\"а\",\n",
    "        'ʊ':\"ью\",\n",
    "        \"θ\":'з',\n",
    "        'ɹ': \"р\",\n",
    "        'ð':\"с\",\n",
    "    }\n",
    "    \n",
    "    extra_dict = {\n",
    "        \"o\":\"о\",\n",
    "        \"a\":\"а\",\n",
    "        \"e\": \"е\",\n",
    "    }\n",
    "\n",
    "    result = \"\"\n",
    "\n",
    "    lst = list(res)\n",
    "    for i,letter in enumerate(lst):\n",
    "        \n",
    "        if letter == 'æ' and i == 0: \n",
    "            result += 'э'\n",
    "            continue\n",
    "                \n",
    "        if letter == 'ɑ' and i == len(lst)-1:\n",
    "            result += \"а\"\n",
    "            continue\n",
    "        \n",
    "        if letter == 'j' and i != len(lst)-1 and lst[i+1] in [\"u\", 'ʊ']: \n",
    "            continue\n",
    "            \n",
    "        if letter == 'ɹ' and i==len(lst)-2 and string.endswith(\"er\"):\n",
    "            result += \"ер\"\n",
    "            continue\n",
    "                \n",
    "        if letter == 'ɹ' and i != 0 and lst[i-1] == \"s\":\n",
    "            result += \"ер\"\n",
    "            continue\n",
    "            \n",
    "        if letter == \"ə\" and i == 0 and string[0] != \"u\":\n",
    "            result += extra_dict[string[0]]\n",
    "            continue\n",
    "            \n",
    "        if letter == \"ə\" and res.endswith(\"iə\") and i == len(lst)-1:\n",
    "            result += \"я\"\n",
    "            continue\n",
    "                \n",
    "        if letter == \"ə\" and res.endswith(\"ə\") and i == len(lst)-1:\n",
    "            result += \"а\"\n",
    "            continue\n",
    "            \n",
    "        if letter == \"w\" and i > 0 and lst[i-1] == \"o\":\n",
    "            continue \n",
    "            \n",
    "        if letter == \"u\" and i!=0 and lst[i-1] == \"j\":\n",
    "            result += \"ью\"\n",
    "            continue\n",
    "                \n",
    "        if letter == \"θ\" and i == len(lst)-1:\n",
    "            result += \"с\"\n",
    "            continue\n",
    "                \n",
    "        else:\n",
    "            if letter in regular_dict:\n",
    "                result += regular_dict[letter]\n",
    "                continue\n",
    "            else:\n",
    "                result += letter\n",
    "                continue \n",
    "    result += \" \"\n",
    "    result = result.strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = Epitran(\"eng-Latn\", ligatures=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посчитаем символьные и лексические расстояния для пар запросов латиницей, переведенной в кириллицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def make_transliteration(df):\n",
    "    df_eng = df[(df.tag==0) & df.agg_types.str.contains(\"L\")]\n",
    "    # соберем только токены на английском\n",
    "    eng_list = set()\n",
    "    qs = df_eng[[\"query\", \"prev_query\"]].values.tolist()\n",
    "    for pair in qs:\n",
    "        for query in pair:\n",
    "            if isinstance(query, str) and re.search(\"[A-Za-z]+\", query):\n",
    "                query = query.split()\n",
    "                for w in query:\n",
    "                    if re.search(\"[A-Za-z]+\", w):\n",
    "                        eng_list.add(w)\n",
    "    print(\"Proportion of pairs with Latin: \", len(qs)/len(df))\n",
    "    eng_list = list(eng_list)\n",
    "    \n",
    "    print(\"Transliterating Latin tokens\")\n",
    "    d = {}\n",
    "    for line in tqdm(eng_list):\n",
    "        try:\n",
    "            d[line] = translate(line)\n",
    "        except:\n",
    "            print(line)\n",
    "            \n",
    "    df[\"q_trans\"] = df[\"query\"].apply(translate_queries)\n",
    "    df[\"prev_q_trans\"] = df[\"prev_query\"].apply(translate_queries)\n",
    "    return df\n",
    "                    \n",
    "\n",
    "def translate_queries(query):\n",
    "    if not isinstance(query, str):\n",
    "        return query\n",
    "        \n",
    "    if not re.search(\"[A-Za-z]+\",query):\n",
    "        return query\n",
    "    \n",
    "    query = query.split()\n",
    "    res = \"\"\n",
    "    for i,w in enumerate(query):\n",
    "        if w in d:\n",
    "            res += d[w] + \" \"\n",
    "        else:\n",
    "            if i == len(query)-1:\n",
    "                res += w\n",
    "                continue\n",
    "            res += w + \" \"\n",
    "    return res\n",
    "\n",
    "def apply_damerau_levenshtein(pairs):\n",
    "    dists = {}\n",
    "    print(\"Counting Damerau-Levenstein distance\")\n",
    "    for i, pair in enumerate(tqdm(pairs)):\n",
    "        if not isinstance(pair[0], str) or not isinstance(pair[1], str):\n",
    "            dists[i] = -1\n",
    "        else:\n",
    "            dists[i] = normalized_damerau_levenshtein_distance(pair[0], pair[1])\n",
    "    return dists\n",
    "\n",
    "def levenstein(df):\n",
    "    df[\"trans_d_lev\"] = -1\n",
    "    pairs = df[[\"q_trans\",\"prev_q_trans\"]].values.tolist()\n",
    "    lev_dists = apply_damerau_levenshtein(pairs)\n",
    "    df[\"trans_d_lev\"] = [lev_dists[i] for i in df.index]\n",
    "    return df\n",
    "\n",
    "def count_overlap(s1, s2):\n",
    "    \n",
    "    s1 = set(s1.split())\n",
    "    s2 = set(s2.split())\n",
    "    denom = (len(s1) + len(s2))/2\n",
    "    if denom == 0:\n",
    "        return 1.0\n",
    "    overlap_ratio = 1 - (len(s1 & s2)/denom)\n",
    "    return overlap_ratio\n",
    "\n",
    "\n",
    "def overlap(df):\n",
    "    pairs = df[[\"q_trans\", \"prev_q_trans\"]].values.tolist()\n",
    "    \n",
    "    overlaps = {}\n",
    "    print(\"Counting overlap score\")\n",
    "    for i, pair in enumerate(tqdm(pairs)):\n",
    "        if not isinstance(pair[0], str) or not isinstance(pair[1], str):\n",
    "            overlaps[i] = -1\n",
    "        else:\n",
    "            overlaps[i] = count_overlap(pair[0], pair[1])\n",
    "    df[\"trans_overlap\"] = [overlaps[i] for i in df.index]   \n",
    "    return df\n",
    "\n",
    "def monge_eklan(df):\n",
    "    pairs = df[[\"q_trans\", \"prev_q_trans\"]].values.tolist()\n",
    "    monge = MongeEklan()\n",
    "    monge_diffs = {}\n",
    "    print(\"Counting Monge-Eklan score\")\n",
    "    for i, pair in enumerate(tqdm(pairs)):\n",
    "        if not isinstance(pair[0], str) or not isinstance(pair[1], str):\n",
    "            monge_diffs[i] = -1\n",
    "        else:\n",
    "            monge_diffs[i] = 1 - monge.score(pair[0], pair[1], m=2)\n",
    "    df[\"trans_monge_eklan\"] = [monge_diffs[i] for i in df.index]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    # рассчитаем близости между транслитерированными запросами для всех датафреймов\n",
    "    paths = [i for i in os.listdir(\".\") if i.endswith(\"edit.tsv\")]\n",
    "    for path in paths:\n",
    "        print(path)\n",
    "        df = read_df(path)        \n",
    "        print(\"Df processed\")\n",
    "        df = make_transliteration(df)\n",
    "        df = levenstein(df)\n",
    "        df = overlap(df)\n",
    "        df = monge_eklan(df)\n",
    "        new_path = path.split(\".\")[0] + \"-trans.tsv\"\n",
    "        df.to_csv(new_path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проставим теги на основе новых расстояний "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def read_df(path):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    return df\n",
    "\n",
    "def make_tags(df, indices):\n",
    "    indices = {i:True for i in indices}\n",
    "    tags = df[\"tag\"].tolist()\n",
    "    for i in tqdm(df.index):\n",
    "        try:\n",
    "            if indices[i]:\n",
    "                tags[i] = 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    df[\"tag\"] = tags\n",
    "    return df\n",
    "\n",
    "def tag(df):\n",
    "    # indices for monge-eklan\n",
    "    indices = df[(df.tag==0) & (df.agg_types.str.contains(\"L\")) & \n",
    "               (df.trans_monge_eklan <= 0.15) & \n",
    "               (df.n_tokens > 1) & \n",
    "                (df.prev_n_tokens > 1)].index.tolist()\n",
    "    print(\"Tag 1 proportion before Monge Eklan\", sum(df.tag)/len(df))\n",
    "    df = make_tags(df, indices)\n",
    "    print(\"Tag 1 proportion after Monge Eklan\", sum(df.tag)/len(df))    \n",
    "    \n",
    "    \n",
    "    # indices for word overlap\n",
    "    indices = df[(df.tag==0) & \n",
    "                 (df.agg_types.str.contains(\"L\")) & \n",
    "                 (df.trans_overlap <= 0.25)].index.tolist()\n",
    "    print(\"Tag 1 proportion before overlap\", sum(df.tag)/len(df))\n",
    "    df = make_tags(df, indices)\n",
    "    print(\"Tag 1 proportion after overlap\", sum(df.tag)/len(df))\n",
    "\n",
    "    \n",
    "    #indices for damerau-levenstein \n",
    "    print(\"Tag 1 proportion before Damerau-Levenstein\", sum(df.tag)/len(df))\n",
    "    df = df.fillna(\"\")\n",
    "    # for 2 and 3 tokens\n",
    "    indices = df[((df.n_tokens > 1) \n",
    "              & (df.prev_n_tokens > 1)) \n",
    "            & (df.n_tokens < 4) \n",
    "             & (df.prev_n_tokens < 4)\n",
    "            & (df.trans_d_lev <= 0.2)\n",
    "            & (~df[\"query\"].str.contains(\"квартир\"))\n",
    "            & (~df[\"prev_query\"].str.contains(\"квартир\"))\n",
    "            & (df.n_char > 5)].index.tolist()\n",
    "    df = make_tags(df, indices)\n",
    "    \n",
    "    # for more than 3 tokens \n",
    "    indices = df[(df.n_tokens > 3) \n",
    "                 & (df.prev_n_tokens > 3) \n",
    "                 & (df.trans_d_lev <= 0.3)\n",
    "                & (~df[\"query\"].str.contains(\"квартир\"))\n",
    "                & (~df[\"prev_query\"].str.contains(\"квартир\"))\n",
    "                & (df.n_char > 5)].index.tolist()\n",
    "    df = make_tags(df, indices)\n",
    "    \n",
    "    # for 1 token \n",
    "    indices = df[(df.n_tokens == 1) \n",
    "       & (df.prev_n_tokens == 1) \n",
    "       & (df.trans_d_lev <= 0.4)  \n",
    "       & (df.prev_type != \"D\") \n",
    "       & (df.type != \"D\") \n",
    "       & (df.n_char > 5)].index.tolist()\n",
    "    df = make_tags(df, indices)\n",
    "    \n",
    "    print(\"Tag 1 proportion after Damerau-Levenstein\", sum(df.tag)/len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    paths = [i for i in os.listdir() if i.endswith(\"trans.tsv\")]\n",
    "    for path in paths:\n",
    "        print(path)\n",
    "        df = read_df(path)\n",
    "        print(\"Df processed\")\n",
    "        new_df = tag(df)\n",
    "        new_path = path.split(\".\")[0] + \"-final.tsv\"\n",
    "        new_df.to_csv(new_path, sep=\"\\t\", index=False)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
